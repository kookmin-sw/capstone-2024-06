{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://files.grouplens.org/datasets/movielens/ml-100k/u.data'\n",
    "df = pd.read_csv(url, sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "ratings_matrix = df.pivot_table(index='user_id', columns='item_id', values='rating').fillna(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FISM(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "        self.query_embeddings = nn.Embedding(num_items, embedding_dim)\n",
    "        self.target_embeddings = nn.Embedding(num_items, embedding_dim)\n",
    "    \n",
    "    def forward(self, user, item_i, item_j, batch_score, n):\n",
    "        user_bias = self.user_bias(user)\n",
    "        item_bias = self.item_bias(item_i)\n",
    "\n",
    "        query_emb = self.query_embeddings(item_j)\n",
    "        target_emb = self.target_embeddings(item_i).unsqueeze(2)\n",
    "        batch_score = batch_score.unsqueeze(1)\n",
    "        \n",
    "        sim_mat = torch.bmm(query_emb, target_emb)\n",
    "        pred = torch.bmm(batch_score, sim_mat).squeeze(-1) / (n - 1)\n",
    "        pred += self.global_bias + user_bias + item_bias\n",
    "        return pred.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FISM_simple(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "        self.item_embeddings = nn.Embedding(num_items, embedding_dim)        \n",
    "    \n",
    "    def forward(self, user, item_i, item_j, batch_score, n):\n",
    "        user_bias = self.user_bias(user)\n",
    "        item_bias = self.item_bias(item_i)\n",
    "\n",
    "        query_emb = self.item_embeddings(item_j)\n",
    "        target_emb = self.item_embeddings(item_i).unsqueeze(2)\n",
    "        batch_score = batch_score.unsqueeze(1)\n",
    "        \n",
    "        sim_mat = torch.bmm(query_emb, target_emb)\n",
    "        pred = torch.bmm(batch_score, sim_mat).squeeze(-1) / (n - 1)\n",
    "        pred += self.global_bias + user_bias + item_bias\n",
    "        return pred.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epoch, model, optimizer, loss_fn, device):\n",
    "    model.to(device)\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss = 0\n",
    "\n",
    "        for user_idx, row in enumerate(ratings_matrix):\n",
    "            nonzero_idx = np.nonzero(row)[0]\n",
    "            nonzero = torch.tensor(row[nonzero_idx], dtype=torch.float32, device=device)\n",
    "            n = len(nonzero_idx)\n",
    "\n",
    "            if n <= 1:\n",
    "                continue\n",
    "\n",
    "            batch_i = []\n",
    "            batch_j = []\n",
    "            batch_score = []\n",
    "            for item_idx in range(n):\n",
    "                target_idx = nonzero_idx[item_idx]\n",
    "                other_idxs = nonzero_idx[:item_idx].tolist() + nonzero_idx[item_idx+1:].tolist()\n",
    "                batch_i.append(target_idx)\n",
    "                batch_j.append(other_idxs)\n",
    "                batch_score.append(row[other_idxs])\n",
    "            \n",
    "            user_idx = torch.tensor([user_idx] * n, device=device)\n",
    "            batch_i = torch.tensor(batch_i, device=device)\n",
    "            batch_j = torch.tensor(batch_j, device=device)\n",
    "            batch_score = torch.tensor(batch_score, dtype=torch.float32, device=device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model.forward(user_idx, batch_i, batch_j, batch_score, n)\n",
    "            loss = loss_fn(pred, nonzero)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        loss_history.append(total_loss)\n",
    "        print(f'Epoch {epoch + 1}, Loss: {total_loss}')\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users, n_items = ratings_matrix.shape\n",
    "fism = FISM(n_users, n_items, 20)\n",
    "optimizer = optim.Adam(fism.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "fism_loss = train(30, fism, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users, n_items = ratings_matrix.shape\n",
    "fism_simple = FISM_simple(n_users, n_items, 20)\n",
    "optimizer = optim.Adam(fism_simple.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "fism_simple_loss = train(30, fism_simple, optimizer, loss_fn, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
